---
import MainLayout from '../../layouts/MainLayout.astro';
import MenuLayout from '../../layouts/MenuLayout.astro';
import Blockquote from '../../components/util/Blockquote.astro';
import List from '../../components/util/List.astro';
import Section from '../../components/util/Section.astro';
import Header from '../../components/util/Header.astro';
---

<MainLayout>
    <MenuLayout menu="getting-started">
        <article class="p-2">
            <header><h1 class="text-3xl">Getting started with AwpakAI</h1></header>
            <section class="pt-2">
                <p>In this first example, we are going to create a simple <b>chat workflow</b> to interact with an LLM (Large Language Model). We will use the <a href="/builder" target="_blank" title="Open graph Builder" class="font-bold">web-based graph builder</a> to define the workflow visually and export it as JSON for execution.</p>
                <p class="p-2"><a href="/builder" target="_blank" title="Open graph Builder" class="font-bold">Click to open the graph builder in new tab</a></p>
            </section>
            <Section>
                <Header>Default Graph Layout</Header>
                <p class="pb-2">When you open the graph builder, you will see <b>three disconnected nodes</b>:</p>
                <ul>
                    <li><b>Entry Node</b> - This is the starting point of the graph and will be executed first.</li>
                    <li><b>Exit Ok</b> - A terminal node indicating that the workflow has completed successfully.</li>
                    <li><b>Exit Err</b> - A terminal node indicating that the workflow has ended with an error.</li>
                </ul>

                <div class="mt-2">
                    <img class="h-60 border" src="/img/docs/initial_graph.png" alt="Initial graph" />
                </div>

                <Header ty="h3">About Exit Nodes</Header>

                <p><b>Exit Ok</b> and <b>Exit Err</b> are <b>terminal nodes</b>: clicking on them does nothing because they have no configurable options.</p>
                <p class="pt-2 pb-1">They are only used to signal how the workflow finishes:</p>

                <p>- Use <b>Exit Ok</b> when execution completes successfully.</p>
                <p>- Use <b>Exit Err</b> when execution fails.</p>

            </Section>
            <Section>
                <Header>Editing the Entry Node</Header>
                
                <p>Clicking on the <b>Entry Node</b> will open a configuration form where you can set its options.</p>
                <p>For this first example, we will configure it to send the user's input to a language model and return its response.</p>
            </Section>
            <Section>
                <Header>Configuring the Node to Call an LLM</Header>
                <p>When you click on the <b>Entry Node</b>, a configuration form will appear.</p>
                <p>One of the most important sections in this form is <b>Node executor</b>, which contains a dropdown labeled <b>Executor type</b>.</p>
                <Header ty="h3">Selecting the Executor Type</Header>
                <p>By default, the executor type is set to <b>Command</b>.</p>
                <p>For our chat example, change it to <b>Agent</b>.</p>
                <p>Once you select <b>Agent</b>, the form updates to show additional options for configuring the AI agent.</p>
                <Header ty="h3">Selecting the Provider</Header>

                <p>In the <b>Provider type</b> dropdown, you can choose from the following supported providers:</p>

                <List items={["Ollama", "OpenAI", "DeepSeek", "Gemini"]} />
                
                <p>For this example, we will use <b>Ollama</b> because it does not require an API key.</p>
                <p>If you choose any of the other providers, an additional field labeled <b>Api key</b> will appear.</p>

                <Blockquote>
                    <p><b>Important:</b> you cannot paste an API key directly in this field.</p>
                    <p>Instead, you must store your API key in an environment variable and specify the <b>name of that variable</b> here.</p>
                </Blockquote>

                <Header ty="h3">Selecting the Model</Header>
                <p>After selecting <b>Ollama</b> as the provider, a new section appears for selecting the model.</p>
                <p>This section has two fields:</p>

                <List items={["1. A dropdown labeled from", "2. A text field"]} />

                <p class="pb-2">This structure, titled <b>DataFrom</b>, is used in many other parts of the form.</p>
                <p>The <b>From</b> dropdown lets you choose where the value comes from:</p>

                <List
                    items={
                        [
                            "Context → use a value stored in the graph context",
                            "Input → use the graph input",
                            "Static → specify a fixed value manually",
                            "*(we will cover other options later)*"
                        ]
                    }
                />
                <p>For our example, choose <b>Static</b> and enter the model name:</p>
                <p class="italic">llama3.1</p>

                <Header ty="h3">Configuring the Prompt</Header>
                <p>Below the model configuration, you will see three sections:</p>
                <List items={["System prompt", "Prompt", "MCP Servers"]} />

                <p>For this example, we only need the <b>Prompt</b> section.</p>

                <p class="pt-2">1. Click the <b>New prompt part</b> button.</p>
                <p>2. A new prompt section appears with:</p>
                
                <List items={[
                    "A text field labeled Prefix",
                    "A DataFrom selector in the middle",
                    "A text field labeled Suffix"
                ]} />

                <p class="pb-1">For our chat workflow:</p>
                <p>- Set <b>DataFrom</b> to <b>Input</b> (we want the user input as the prompt).</p>
                <p>- Leave <b>Prefix</b> and <b>Suffix</b> empty (they are optional and allow adding extra text before or after the main input).</p>

                <Blockquote>
                    <p><b>Tip:</b> You can create multiple prompt parts by clicking <b>New prompt part</b> again.</p>
                    <p>For example, you could concatenate user input with a value from the context.</p>
                </Blockquote>

                <Header ty="h3">Enabling Chat Behavior</Header>
                <p>At the bottom of the form, you will see two checkboxes:</p>
                <List items={[
                    "Save history → keeps the conversation history across graph executions.",
                    "Streaming response → enables streaming responses from the LLM."
                ]} />
                <p>For a chat use case <b>enable both options</b></p>
                <p class="pt-2">With these settings, our entry node is now configured to act as a chat agent connected to an LLM.</p>
                <p>Next, we will connect the nodes to complete the workflow.</p>
            </Section>
            <Section>
                <Header>Connecting the Nodes</Header>
                <p>Now that the entry node is configured, let’s return to the graph view to connect the nodes.</p>
                <Header ty="h3">Switching to Graph View</Header>
                <p>At the top of the page, you will see a dropdown labeled <b>Options</b>.</p>
                <p>Open it and select <b>Graph view</b>.</p>

                <p>This brings us back to the default view, where we still see the three disconnected nodes:</p>

                <List items={["Entry Node", "Exit Ok", "Exit Err"]} />

                <Header ty="h3">Entering Edit Mode</Header>
                <p>To create connections between nodes, we first need to enable <b>Edit Mode</b>:</p>
                <List items={[
                    "1. Click the button at the top of the screen labeled Start edit mode",
                    "2. Once activated, the label changes to End edit mode."
                ]} />

                <Header ty="h3">Creating a Connection</Header>

                <p>In our simple chat example, we only need to connect the <b>Entry Node</b> to <b>Exit Ok</b>:</p>
                
                <List items={[
                    "1. Click on the Entry Node and, while holding the click, drag towards the Exit Ok node.",
                    "2. When your cursor hovers over Exit Ok, release the mouse.",
                    "3. A connection arrow will appear between the nodes."
                ]} />

                <p>Once connected, click <b>End edit mode</b> to prevent accidental changes.</p>

                <Header ty="h3">Saving the Graph as JSON</Header>

                <p>With the nodes connected, the last step is to export the graph:</p>
                
                <List items={[
                    "1. Open the Options dropdown again.",
                    "2. Select Save JSON."
                ]} />

                <p>This will download a JSON file representing the workflow we have just built.</p>
            </Section>
            <Section>
                <Header>Running the Graph</Header>
                
                <p>While you can run graphs directly using the Rust library <b>awpak-ai</b>, there is also a <b>command-line application</b> available that allows <b>anyone—even non-programmers—to execute graphs</b>.</p>

                <p>The CLI application is called <b>awpak-ai-cmd-client</b>.</p>
                <p>You can find instructions to install it in its <a class="font-bold" href="https://github.com/afuentesan/awpak-tui/tree/main/awpak-ai-cmd-client" title="awpak-ai-cmd-client Github repository">GitHub repository</a>.</p>

                <Header ty="h3">Running the Example</Header>

                <p>For our simple chat workflow, run the following command:</p>

                <code>awpak-ai-cmd-client --path="/path_to_graph.json" --trace="agent_stream" --chat</code>

                <Header ty="h3">Parameters:</Header>

                <List items={[
                    "`--path` → Path to the JSON file representing your graph.",
                    "`--trace` → Set to `agent_stream` to enable streaming output from the agent (this works because we enabled Streaming response in the graph configuration).",
                    "`--chat` → Starts an interactive console prompt. Type your messages and press Enter to send them to the graph input."
                ]} />
                
                <p>This will run the workflow we created, using the LLM agent, and display streaming responses directly in your terminal.</p>
            </Section>
        </article>
    </MenuLayout>
</MainLayout>

