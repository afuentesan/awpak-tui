---
import MainLayout from '../../layouts/MainLayout.astro';
import MenuLayout from '../../layouts/MenuLayout.astro';
import Blockquote from '../../components/util/Blockquote.astro';
import List from '../../components/util/List.astro';
import Section from '../../components/util/Section.astro';
import Header from '../../components/util/Header.astro';
import Link from '../../components/util/Link.astro';
import Img from '../../components/util/Img.astro';
---

<MainLayout>
    <MenuLayout menu="getting-started">
        <article class="p-2">
            <header>
                <Header ty="h1">
                    Getting started with AwpakAI
                </Header>
            </header>
            <section class="pt-2">
                <p>In this first example, we are going to create a simple <b>chat workflow</b> to interact with an LLM (Large Language Model). We will use the <Link href="/builder" target="_blank" title="Open graph Builder">web-based graph builder</Link> to define the workflow visually and export it as JSON for execution.</p>
                <p class="p-2"><Link href="/builder" target="_blank" title="Open graph Builder">Click to open the graph builder in new tab</Link></p>
            </section>
            <Section>
                <Header>Default Graph Layout</Header>
                <p class="pb-2">When you open the graph builder, you will see <b>three disconnected nodes</b>:</p>
                <List items={[]}>
                    <li><b>Entry Node</b> - This is the starting point of the graph and will be executed first.</li>
                    <li><b>Exit Ok</b> - A terminal node indicating that the workflow has completed successfully.</li>
                    <li><b>Exit Err</b> - A terminal node indicating that the workflow has ended with an error.</li>
                </List>

                <div class="mt-2">
                    <Img height="h-60" src="/img/docs/initial_graph.png" alt="Initial graph" />
                </div>

                <Header ty="h3">About Exit Nodes</Header>

                <p><b>Exit Ok</b> and <b>Exit Err</b> are <b>terminal nodes</b>: clicking on them does nothing because they have no configurable options.</p>
                <p class="pt-2 pb-1">They are only used to signal how the workflow finishes:</p>

                <p>- Use <b>Exit Ok</b> when execution completes successfully.</p>
                <p>- Use <b>Exit Err</b> when execution fails.</p>

            </Section>
            <Section>
                <Header>Editing the Entry Node</Header>
                
                <p>Clicking on the <b>Entry Node</b> will open a configuration form where you can set its options.</p>
                <p>For this first example, we will configure it to send the user's input to a language model and return its response.</p>
            </Section>
            <Section>
                <Header>Configuring the Node to Call an LLM</Header>
                <p>When you click on the <b>Entry Node</b>, a configuration form will appear.</p>
                <p>One of the most important sections in this form is <b>Node executor</b>, which contains a dropdown labeled <b>Executor type</b>.</p>
                <div class="mt-2">
                    <Img height="h-30" src="/img/docs/executor_type.png" alt="Executor type" />
                </div>
                <Header ty="h3">Selecting the Executor Type</Header>
                <p>By default, the executor type is set to <b>Command</b>.</p>
                <p>For our chat example, change it to <b>Agent</b>.</p>
                <p>Once you select <b>Agent</b>, the form updates to show additional options for configuring the AI agent.</p>
                <Header ty="h3">Selecting the Provider</Header>

                <p>In the <b>Provider type</b> dropdown, you can choose from the following supported providers:</p>

                <List items={["Ollama", "OpenAI", "DeepSeek", "Gemini"]} />
                
                <p>For this example, we will use <b>Ollama</b> because it does not require an API key.</p>

                <div class="mt-2 mb-2">
                    <Img height="h-30" src="/img/docs/provider_selector.png" alt="Provider selector" />
                </div>

                <p>If you choose any of the other providers, an additional field labeled <b>Api key</b> will appear.</p>

                <Blockquote>
                    <p><b>Important:</b> you cannot paste an API key directly in this field.</p>
                    <p>Instead, you must store your API key in an environment variable and specify the <b>name of that variable</b> here.</p>
                </Blockquote>

                <Header ty="h3">Selecting the Model</Header>
                <p>After selecting <b>Ollama</b> as the provider, a new section appears for selecting the model.</p>
                <p>This section has two fields:</p>

                <List items={["A dropdown labeled from", "A text field"]} />

                <div class="mt-2 mb-2">
                    <Img height="h-40" src="/img/docs/data_from.png" alt="DataFrom" />
                </div>

                <p class="pb-2">This structure, titled <b>DataFrom</b>, is used in many other parts of the form.</p>
                <p>The <b>From</b> dropdown lets you choose where the value comes from:</p>

                <List
                    items={[]}
                >
                    <li><b>Context</b>: use a value stored in the graph context</li>
                    <li><b>Input</b>: use the graph input</li>
                    <li><b>Static</b>: specify a fixed value manually</li>
                    <li>*(we will cover other options later)*</li>
                </List>

                <p>For our example, choose Static and enter the model name: <span class="italic">llama3.1</span>.</p>
                <p>You can replace llama3.1 with any model you have installed locally.</p>
                <p>Since this example uses <b>Ollama</b> as the provider, make sure that Ollama is running on your local machine before executing the graph.</p>

                <Header ty="h3">Configuring the Prompt</Header>
                <p>Below the model configuration, you will see three sections:</p>
                <List items={["System prompt", "Prompt", "MCP Servers"]} />

                <p>For this example, we only need the <b>Prompt</b> section.</p>

                <div class="mt-2 mb-2">
                    <Img height="h-25" src="/img/docs/prompt.png" alt="Prompt" />
                </div>

                <p class="pt-2">1. Click the <b>New prompt part</b> button.</p>
                <p>2. A new prompt section appears with:</p>
                
                <List items={[
                    "A text field labeled Prefix",
                    "A DataFrom selector in the middle",
                    "A text field labeled Suffix"
                ]} />

                <div class="mt-2 mb-2">
                    <Img height="h-96" src="/img/docs/prompt_part.png" alt="Prompt part" />
                </div>

                <p class="pb-1">For our chat workflow:</p>
                <p>- Set <b>DataFrom</b> to <b>Input</b> (we want the user input as the prompt).</p>
                <p>- Leave <b>Prefix</b> and <b>Suffix</b> empty (they are optional and allow adding extra text before or after the main input).</p>

                <Blockquote>
                    <p><b>Tip:</b> You can create multiple prompt parts by clicking <b>New prompt part</b> again.</p>
                    <p>For example, you could concatenate user input with a value from the context.</p>
                </Blockquote>

                <Header ty="h3">Enabling Chat Behavior</Header>
                <p>At the bottom of the form, you will see two checkboxes:</p>
                <List items={[
                    "Save history → keeps the conversation history across graph executions.",
                    "Streaming response → enables streaming responses from the LLM."
                ]} />

                <div class="mt-2 mb-2">
                    <Img height="h-12" src="/img/docs/streaming_and_save_history.png" alt="Streaming response and save history" />
                </div>

                <p>For a chat use case <b>enable both options</b></p>
                <p class="pt-2">With these settings, our entry node is now configured to act as a chat agent connected to an LLM.</p>
                <p>Next, we will connect the nodes to complete the workflow.</p>
            </Section>
            <Section>
                <Header>Connecting the Nodes</Header>
                <p>Now that the entry node is configured, let's return to the graph view to connect the nodes.</p>
                <Header ty="h3">Switching to Graph View</Header>
                <p>At the top of the page, you will see a dropdown labeled <b>Options</b>.</p>
                <p>Open it and select <b>Graph view</b>.</p>

                <div class="mt-2 mb-2">
                    <Img height="h-40" src="/img/docs/options_menu.png" alt="Options dropdown" />
                </div>

                <p>This brings us back to the default view, where we still see the three disconnected nodes:</p>

                <List items={["Entry Node", "Exit Ok", "Exit Err"]} />

                <Header ty="h3">Entering Edit Mode</Header>
                <p>To create connections between nodes, we first need to enable <b>Edit Mode</b>:</p>
                <List items={[
                    "Click the button at the top of the screen labeled Start edit mode",
                    "Once activated, the label changes to End edit mode."
                ]} list_type="ol" />

                <div class="mt-2 mb-2">
                    <Img height="h-12" src="/img/docs/start_edit_mode.png" alt="Start edit mode" />
                </div>
                
                <div class="mt-2 mb-2">
                    <Img height="h-12" src="/img/docs/end_edit_mode.png" alt="End edit mode" />
                </div>

                <Header ty="h3">Creating a Connection</Header>

                <p>In our simple chat example, we only need to connect the <b>Entry Node</b> to <b>Exit Ok</b>:</p>
                
                <List items={[
                    "Click on the Entry Node and, while holding the click, drag towards the Exit Ok node.",
                    "When your cursor hovers over Exit Ok, release the mouse.",
                    "A connection arrow will appear between the nodes."
                ]} list_type="ol" />

                <p>Once connected, click <b>End edit mode</b> to prevent accidental changes.</p>
                
                <div class="mt-2 mb-2">
                    <Img height="h-30" src="/img/docs/connected_graph.png" alt="Connect entry node with Exit Ok" />
                </div>

                <Header ty="h3">Saving the Graph as JSON</Header>

                <p>With the nodes connected, the last step is to export the graph:</p>
                
                <List items={[
                    "Open the Options dropdown again.",
                    "Select Save JSON."
                ]} list_type="ol" />

                <p>This will download a JSON file representing the workflow we have just built.</p>
            </Section>
            <Section>
                <Header>Running the Graph</Header>
                
                <p>While you can run graphs directly using the Rust library <b>awpak-ai</b>, there is also a <b>command-line application</b> available that allows <b>anyone—even non-programmers—to execute graphs</b>.</p>

                <p>The CLI application is called <b>awpak-ai-cmd-client</b>.</p>
                <p>You can find instructions to install it in its <Link href="https://github.com/afuentesan/awpak-tui/tree/main/awpak-ai-cmd-client" title="awpak-ai-cmd-client Github repository" target="_blank">GitHub repository</Link>.</p>

                <Header ty="h3">Running the Example</Header>

                <p class="pb-2">For our simple chat workflow, run the following command:</p>

                <p class="border border-gray-200 shadow-sm dark:border-gray-500 p-1 bg-gray-100 rounded-lg dark:bg-gray-700 text-sm text-gray-700 dark:text-gray-200">
                    <code>awpak-ai-cmd-client --path="/path_to_graph.json" --trace="agent_stream" --chat</code>
                </p>

                <Header ty="h3">Parameters:</Header>

                <List items={[]}>
                    <li><b>--path</b>: Path to the JSON file representing your graph.</li>
                    <li><b>--trace</b>: Set to `agent_stream` to enable streaming output from the agent (this works because we enabled Streaming response in the graph configuration).</li>
                    <li><b>--chat</b>: Starts an interactive console prompt. Type your messages and press Enter to send them to the graph input.</li>
                </List>
                
                <p>This will run the workflow we created, using the LLM agent, and display streaming responses directly in your terminal.</p>

                <div class="mt-2 mb-4">
                    <Img height="h-10" src="/img/docs/final_ollama_chat.png" alt="Ollama chat execution" />
                </div>
            </Section>
            <Section>
                <Header>Next Steps</Header>
                <p>Now that you've built your first graph and successfully run it, you might be wondering what other options are available to customize its behavior.</p>
                <p>In the next section, we'll explore the <b>initial configuration options</b> of a graph in more detail — from setting the expected input type, to defining an initial context, and deciding whether to preserve that context between runs.</p>

                <p><Link href="/docs/graph-config" title="Learn more about graph configuration options">Learn more about graph configuration options →</Link></p>

            </Section>
        </article>
    </MenuLayout>
</MainLayout>

